\documentclass[11pt]{article}
\usepackage{euscript}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{xspace}
\usepackage{color}
\usepackage{url}
\usepackage{enumerate}
\usepackage{subfigure}

%%%%%%%  For drawing trees  %%%%%%%%%
\usepackage{tikz}
\usetikzlibrary{calc, shapes, backgrounds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\textheight}{9in}
\setlength{\topmargin}{-0.600in}
\setlength{\headheight}{0.2in}
\setlength{\headsep}{0.250in}
\setlength{\footskip}{0.5in}
\flushbottom
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\columnsep}{2pc}
\setlength{\parindent}{0em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\eps}{\varepsilon}

\renewcommand{\c}[1]{\ensuremath{\EuScript{#1}}}
\renewcommand{\b}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\s}[1]{\textsf{#1}}

\newcommand{\E}{\textbf{\textsf{E}}}
\renewcommand{\Pr}{\textbf{\textsf{Pr}}}

\newenvironment{pkl}{%
\begin{itemize}%
%\vspace{-\topsep}%
\setlength\itemsep{-0.5\parskip}%
\setlength\parsep{0in}%
}{%
%\vspace{-\topsep}%
\end{itemize}}

\newenvironment{enu}{%
\begin{enumerate}%
\vspace{-\topsep}%
\setlength\itemsep{-\parskip}%
\setlength\parsep{0in}%
}{%
\vspace{-\topsep}%
\end{enumerate}}
\allowdisplaybreaks


\title{Readme for reproducibility submission of SIGMOD'21 paper ID 68}
\author{}
\date{}

\begin{document}
\maketitle

\section{Source code info}

{\bf Repository:} https://github.com/Yanqing-UTAH/ATTPCode\\
{\bf Programming Langauge:} mainly C/C++, some of the scripts are written in
Python\\
{\bf Required software/library:} gcc/g++ >= 8 (requires support for
-std=gnu++17), make, lapack and lapacke, blas and cblas, fftw3,
python3, numpy, scipy, sklearn and the common shell programs (bash,
grep, sed, and etc.).\\
{\bf Optional software/library:} for plotting figures: jupyter
notebook, python3
matplotlib, pandas, numpy; for recreating ``configure'' script if that
does not work in your environment: m4, autoconf, autoheader.  \\
{\bf Hardware requirement:} we assume the architecture implements <= 48-bit
virtual address space and always uses x86-64 canonical addresses.
That's the case with any Intel/AMD processor and Linux kernel
combination other than the recent Linux kernels that are configured to
allocate memory beyond 47-bit user address space on the processors
that support 5-level paging (e.g., Intel Ice Lake) as of right now
(2021). 

\section{Test environment}
{\bf Software environment:}
\begin{pkl}
	\item Ubuntu 18.04.6 LTS
    \item GNU make 4.1
	\item GCC/G++ 8.3.0
	\item liblapack-dev 3.7.1
	\item liblapacke-dev 3.7.1
	\item libblas-dev 3.7.1
	\item libatlas-base-dev 3.10.3
	\item libfftw3-dev 3.3.7
    \item python3 3.6.9
    \item sklearn 0.22.2
    \item scipy 1.4.1
    \item numpy 1.19.0
\end{pkl}
Note that these are the ones installed at the
time we performed the experiments but it should be ok if you use newer
versions. 

\newpage
{\bf Hardware environment:}

\vspace{1mm}
\begin{tabular}{|l|l|l|}
	\hline
	Node type &  1 & 2 \\\hline
	CPU & Intel Core i7-3820 & Intel Xeon E5-1650 v3 \\\hline
	CPU Frequency & 3.6 GHz & 3.50 GHz \\\hline
	L1 Cache & 32KB + 32 KB & 32KB + 32 KB\\\hline
	L2 Cache & 256 KB & 256 KB\\\hline
	L3 Cache & 10 MB (shared) & 15 MB (shared) \\\hline
	Memory & 64GB (DDR3-1600 x 8) & 128GB (DDR4-2133 x 4) \\\hline
	Secondary storage & WD HDD 7200RPM 2TB & Seagate HDD 7200RPM 1TB
	\\\hline
	Network & not used & not used \\\hline
\end{tabular}
\vspace{1mm}

The experiments are single-threaded and we only
launch one experiement at a time on a single node so the number of
cores or hyperthreading is irrelevant to our purpose. Cache size is
also irrelevant for our purpose as our data structure size far exceeds
even the L3 cache size (listed below just for reference). On the other
hand, we have 10 type-1 nodes and 6 type-2 nodes and we may launch any
of the experiments on any one of them depending on the availability.
We made sure that there were no computation or I/O heavy programs
running concurrently.

\section{Preparing datasets}

We have two scripts for creating the datasets: 1) the datasets based
on the FIFA World Cup 98 website access logs for the ATTP/BITP heavy
hitter experiments; 2) the synthetic datasets for the ATTP
frequent direction experiments. 

For the world-cup datasets (for the ATTP/BITP heavy hitters), run:
\begin{verbatim}
$ ./data_proc/world-cup/prepare_data.sh
\end{verbatim}
It takes about xxx to generate all the datasets. In case the website
hosting the raw logs is unreachable, please contact Zhuoyue
(zzhao35@buffalo.edu) for our own copy.

For the matrix datasets (for the ATTP frequent directions): run:
\begin{verbatim}
$ ./data_proc/gen_mat_data.sh
\end{verbatim}
It takes less than 12 minutes to generate all the three datasets (small,
medium and large). If you only want one or some of the three datasets,
specify its name as a command line argument to the script (e.g.,
$\texttt{./data\_proc/gen\_mat\_data.sh small}$)

All the generated data are put into the data/ directory, see
data/README.md for descriptions.

\section{Building the code}
If you're using Ubuntu, you should be able to use the following to
prepare all the required prerequisites:
\begin{verbatim}
$ sudo apt install gcc g++ make liblapacke-dev libatlas-base-dev \
> libfftw3-dev python3 python3-pip
$ pip3 install sklearn scipy numpy
\end{verbatim}

To build the code:
\begin{verbatim}
$ ./configure
$ make
\end{verbatim}

\section{Running the experiments}

\section{Plotting the figures}

You'll need jupyter notebook, matplotlib, numpy and pandas to plot the
figures. The scripts provided in plot/ work on my local WSL 2.0
installation with Ubuntu 20.04 LTS, python 3.8.10, matplotlib 3.4.3,
numpy 1.21.3, pandas 1.3.4 and jupter notebook 6.0.3.
\begin{verbatim}
$ sudo apt install python3 python3-pip jupyter-notebook
$ pip3 install matplotlib numpy pandas
\end{verbatim}

The following table lists which notebook you should run to generate
specific figures as well as the expected input file. These scripts
also generates pdf files for the figures in the plot/ directory.

{\scriptsize
\begin{tabular}{|l|l|l|}
    \hline
    Notebook & Figures & Input \\\hline
    $\texttt{HH\_ATTP\_clientid.ipynb}$ & 2, 3(left), 4 &
    $\texttt{filtered\_logs/client\_id\_attp\_filtered\_combined.txt}$\\\hline
    $\texttt{HH\_ATTP\_objectid.ipynb}$ & 3(right), 5, 6 &
    $\texttt{filtered\_logs/object\_id\_attp\_filtered\_combined.txt}$\\\hline
    $\texttt{HH\_BITP\_clientid.ipynb}$ & 7, 8(left), 9 &
    $\texttt{filtered\_logs/client\_id\_bitp\_filtered\_combined.txt}$\\\hline
    $\texttt{HH\_BITP\_objectid.ipynb}$ & 8(right), 10, 11&
    $\texttt{filtered\_logs/object\_id\_bitp\_new\_filtered\_combined.txt}$\\\hline
    $\texttt{MAT\_ATTP\_small.ipynb}$ & 12(a), 13(left), 14&
    $\texttt{filtered\_logs/ms\_small\_attp\_filtered\_combined.txt}$ \\\hline
    $\texttt{MAT\_ATTP\_medium.ipynb}$ & 12(b), 13(right), 15&
    $\texttt{filtered\_logs/ms\_medium\_attp\_filtered\_combined.txt}$ \\\hline
    $\texttt{MAT\_ATTP\_big.ipynb}$ & 12(c), 16&
    $\texttt{filtered\_logs/ms\_big\_attp\_filtered\_combined.txt}$ \\\hline
    $\texttt{scalability\_test\_client\_id\_ATTP.ipynb}$ & 1 &
    $\texttt{scalability\_logs/scalability-test-client-id.log}$ \\\hline
\end{tabular}
}


\end{document}

